Rcpp::sourceCpp('src/export_all_funcs.cpp')
source('~/Desktop/kaggle_gpu/add_GITHUB/textTinyR/R/utils.R')
library(Matrix)
PATH = "/home/lampros/Desktop/kaggle_gpu/add_GITHUB/textTinyR/tests/testthat/term_matrix_file.csv"
docs = as.vector(read.csv(PATH, header = FALSE, stringsAsFactors = F)[, 1])
init = sparse_term_matrix$new(vector_data = docs, file_data = NULL, document_term_matrix = TRUE)
res = init$Term_Matrix(sort_terms = FALSE, to_lower = TRUE, to_upper = FALSE, utf_locale = "", remove_char = "", remove_punctuation_string = FALSE, remove_punctuation_vector = FALSE,
remove_numbers = FALSE, trim_token = FALSE, split_string = TRUE, split_separator = " \r\n\t.,;:()?!//", remove_stopwords = FALSE, language = "english",
min_num_char = 1, max_num_char = Inf, stemmer = NULL, min_n_gram = 1, max_n_gram = 1, skip_n_gram = 1, skip_distance = 0, n_gram_delimiter = " ",
stemmer_ngram = 1, stemmer_gamma = 0.0, stemmer_truncate = 1, stemmer_batches = 1, print_every_rows = 100, normalize = NULL, tf_idf = FALSE,
threads = 1, verbose = FALSE)
a = adj_Sparsity(as.vector(lst$COLS), as.vector(lst$ROWS), as.vector(lst$COUNT), as.vector(lst$TERMS), sparsity_thresh = 0.8)
source('~/Desktop/kaggle_gpu/add_GITHUB/textTinyR/R/utils.R')
library(Matrix)
PATH = "/home/lampros/Desktop/kaggle_gpu/add_GITHUB/textTinyR/tests/testthat/term_matrix_file.csv"
docs = as.vector(read.csv(PATH, header = FALSE, stringsAsFactors = F)[, 1])
init = sparse_term_matrix$new(vector_data = docs, file_data = NULL, document_term_matrix = TRUE)
res = init$Term_Matrix(sort_terms = FALSE, to_lower = TRUE, to_upper = FALSE, utf_locale = "", remove_char = "", remove_punctuation_string = FALSE, remove_punctuation_vector = FALSE,
remove_numbers = FALSE, trim_token = FALSE, split_string = TRUE, split_separator = " \r\n\t.,;:()?!//", remove_stopwords = FALSE, language = "english",
min_num_char = 1, max_num_char = Inf, stemmer = NULL, min_n_gram = 1, max_n_gram = 1, skip_n_gram = 1, skip_distance = 0, n_gram_delimiter = " ",
stemmer_ngram = 1, stemmer_gamma = 0.0, stemmer_truncate = 1, stemmer_batches = 1, print_every_rows = 100, normalize = NULL, tf_idf = FALSE,
threads = 1, verbose = FALSE)
res
tmp = init$Term_Matrix_Adjust(sparsity_thresh = 0.8)
tmp
tmp = init$Term_Matrix_Adjust(sparsity_thresh = 0.1)
setwd('/home/lampros/Desktop/kaggle_gpu/add_GITHUB/textTiny/textTinyR')
Rcpp::compileAttributes(verbose = T)
setwd('/home/lampros/Desktop/kaggle_gpu/add_GITHUB/textTiny/')
system("R CMD build textTinyR")
system("R CMD build textTinyR")
system("R CMD build textTinyR")
system("R CMD check --as-cran textTinyR_1.0.3.tar.gz")
system("R CMD INSTALL --clean textTinyR_1.0.3.tar.gz")
Rcpp::sourceCpp('src/export_all_funcs.cpp')
source('~/Desktop/kaggle_gpu/add_GITHUB/textTinyR/R/utils.R')
train = read.csv('/home/lampros/Desktop/kaggle_gpu/quora/train.csv', stringsAsFactors = F)
dim(train); head(train); str(train)
test = read.csv('/home/lampros/Desktop/kaggle_gpu/quora/test.csv', stringsAsFactors = F)
dim(test); head(test); str(test)
dat = c(train$question1, train$question2, test$question1, test$question2)
length(dat)
tok = tokenize_transform_vec_docs(object = dat[1:10], as_token = FALSE,
to_lower = T, to_upper = FALSE, remove_punctuation_vector = T,
remove_numbers = FALSE, trim_token = T, split_string = T,
split_separator = " \r\n\t.,;:()?!//", remove_stopwords = T,
language = "english", min_num_char = 1, max_num_char = Inf,
stemmer = NULL, min_n_gram = 1, max_n_gram = 1, skip_n_gram = 1,
skip_distance = 0, n_gram_delimiter = " ", concat_delimiter = NULL,
path_2folder = PATH, stemmer_ngram = 4, stemmer_gamma = 0,
stemmer_truncate = 3, stemmer_batches = 1, threads = 1,
vocabulary_path_file = NULL, verbose = T)
PATH = '/home/lampros/Downloads'
tok = tokenize_transform_vec_docs(object = dat[1:10], as_token = FALSE,
to_lower = T, to_upper = FALSE, remove_punctuation_vector = T,
remove_numbers = FALSE, trim_token = T, split_string = T,
split_separator = " \r\n\t.,;:()?!//", remove_stopwords = T,
language = "english", min_num_char = 1, max_num_char = Inf,
stemmer = NULL, min_n_gram = 1, max_n_gram = 1, skip_n_gram = 1,
skip_distance = 0, n_gram_delimiter = " ", concat_delimiter = NULL,
path_2folder = PATH, stemmer_ngram = 4, stemmer_gamma = 0,
stemmer_truncate = 3, stemmer_batches = 1, threads = 1,
vocabulary_path_file = NULL, verbose = T)
PATH = '/home/lampros/Downloads/'
tok = tokenize_transform_vec_docs(object = dat[1:10], as_token = FALSE,
to_lower = T, to_upper = FALSE, remove_punctuation_vector = T,
remove_numbers = FALSE, trim_token = T, split_string = T,
split_separator = " \r\n\t.,;:()?!//", remove_stopwords = T,
language = "english", min_num_char = 1, max_num_char = Inf,
stemmer = NULL, min_n_gram = 1, max_n_gram = 1, skip_n_gram = 1,
skip_distance = 0, n_gram_delimiter = " ", concat_delimiter = NULL,
path_2folder = PATH, stemmer_ngram = 4, stemmer_gamma = 0,
stemmer_truncate = 3, stemmer_batches = 1, threads = 1,
vocabulary_path_file = NULL, verbose = T)
tok
tok = tokenize_transform_vec_docs(object = dat[1:10], as_token = T,
to_lower = T, to_upper = FALSE, remove_punctuation_vector = T,
remove_numbers = FALSE, trim_token = T, split_string = T,
split_separator = " \r\n\t.,;:()?!//", remove_stopwords = T,
language = "english", min_num_char = 1, max_num_char = Inf,
stemmer = NULL, min_n_gram = 1, max_n_gram = 1, skip_n_gram = 1,
skip_distance = 0, n_gram_delimiter = " ", concat_delimiter = NULL,
path_2folder = PATH, stemmer_ngram = 4, stemmer_gamma = 0,
stemmer_truncate = 3, stemmer_batches = 1, threads = 1,
vocabulary_path_file = NULL, verbose = T)
tok
tok = tokenize_transform_vec_docs(object = dat[1:10], as_token = T,
to_lower = T, to_upper = FALSE, remove_punctuation_vector = T,
remove_numbers = FALSE, trim_token = T, split_string = T,
split_separator = " \r\n\t.,;:()?!//", remove_stopwords = T,
language = "english", min_num_char = 1, max_num_char = Inf,
stemmer = NULL, min_n_gram = 1, max_n_gram = 1, skip_n_gram = 1,
skip_distance = 0, n_gram_delimiter = " ", concat_delimiter = NULL,
path_2folder = "", stemmer_ngram = 4, stemmer_gamma = 0,
stemmer_truncate = 3, stemmer_batches = 1, threads = 1,
vocabulary_path_file = NULL, verbose = T)
tok
tok = tokenize_transform_vec_docs(object = dat[1:10], as_token = F,
to_lower = T, to_upper = FALSE, remove_punctuation_vector = T,
remove_numbers = FALSE, trim_token = T, split_string = T,
split_separator = " \r\n\t.,;:()?!//", remove_stopwords = T,
language = "english", min_num_char = 1, max_num_char = Inf,
stemmer = NULL, min_n_gram = 1, max_n_gram = 1, skip_n_gram = 1,
skip_distance = 0, n_gram_delimiter = " ", concat_delimiter = NULL,
path_2folder = "", stemmer_ngram = 4, stemmer_gamma = 0,
stemmer_truncate = 3, stemmer_batches = 1, threads = 1,
vocabulary_path_file = NULL, verbose = T)
tok
